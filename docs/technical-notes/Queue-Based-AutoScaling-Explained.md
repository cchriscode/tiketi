# 큐 기반 Auto Scaling 동작 원리 완전 설명
> **대기열이 어떻게 트래픽을 버퍼링하고, EC2가 어떻게 처리하는지**

---

## 🤔 핵심 질문들

1. **갑자기 트래픽이 몰리면?** EC2 추가에 3~5분 걸리는데 그 사이는?
2. **대기열이 정확히 뭐하는 건가?** 트래픽을 막는 건가, 저장하는 건가?
3. **EC2가 늘어나면?** 대기열에서 어떻게 나눠주나?
4. **대기열에 있는 사람들은?** 그냥 기다리기만 하나?

**답: 대기열은 "은행 번호표" 같은 거예요!**

---

## 📖 은행 번호표 비유로 이해하기

### 상황 1: 평소 (사람 적음)

```
은행 창구: 2개 (EC2 2대)
손님: 10명

┌─────────┐  ┌─────────┐
│ 창구 1  │  │ 창구 2  │
│ 👤 처리중│  │ 👤 처리중│
└─────────┘  └─────────┘

대기: 👤👤👤👤👤👤👤👤

→ 번호표 없이 바로 창구로!
→ 대기열 시스템 꺼짐
```

### 상황 2: 갑자기 사람 몰림 (피크 타임)

```
은행 창구: 2개 (EC2 2대)
손님: 10,000명!

┌─────────┐  ┌─────────┐
│ 창구 1  │  │ 창구 2  │
│ 👤 처리중│  │ 👤 처리중│
└─────────┘  └─────────┘

밖에 대기: 👥👥👥👥👥👥👥👥👥... (9,998명)

→ 이러면 은행이 터져요!
→ 번호표 시스템 가동!
```

**번호표 시스템 (대기열):**
```
1번: 창구 1 → 처리 중
2번: 창구 2 → 처리 중
3번: 대기 (다음 차례)
4번: 대기
5번: 대기
...
10,000번: 대기

→ 질서 있게 줄 서기!
→ 은행은 안 터지고, 사람들은 순번 보장!
```

**동시에 창구 추가 요청:**
```
은행장: "손님이 너무 많네? 창구 8개 더 열어!"
직원들: "네! 준비 중..." (3~5분 소요)

3~5분 후:
┌─────────┐ ┌─────────┐ ... ┌─────────┐
│ 창구 1  │ │ 창구 2  │     │ 창구 10 │
│ 👤 처리중│ │ 👤 처리중│     │ 👤 처리중│
└─────────┘ └─────────┘     └─────────┘

→ 이제 10배 빠르게 처리!
→ 대기열이 빠르게 줄어듦!
```

---

## 🎫 TIKETI 실제 시나리오

### 타임라인: 티켓 오픈 시각 (20:00)

```
시간        사용자      대기열      EC2    처리속도      상황
────────────────────────────────────────────────────────────
19:50      500명       0명        2대    1,000명/분    정상
           ↓
           바로 입장 ✅
           대기열 시스템 꺼져있음

────────────────────────────────────────────────────────────
19:58      10,000명!   8,000명    2대    1,000명/분    위험!
           ↓
           임계값 초과! (1,000명)

           앞 1,000명: 바로 입장 ✅
           나머지 9,000명: 대기열로 ⏳

           🚨 Lambda 감지!
           "대기열 8,000명? Auto Scaling 발동!"

────────────────────────────────────────────────────────────
20:00      15,000명    13,000명   2대    1,000명/분    EC2 추가 중...
           ↓                      (준비 중: +5대)

           새 사용자 5,000명 더 몰림!
           → 전부 대기열로

           처리:
           2대가 분당 1,000명씩 처리
           대기열: 13,000 → 12,000 → 11,000...

           ⏱️ 3분 경과...

────────────────────────────────────────────────────────────
20:03      15,000명    10,000명   7대!   3,500명/분    개선!
           ↓
           🎉 EC2 5대 추가 완료!

           처리 속도 3.5배 증가:
           7대 × 500명/분 = 3,500명/분

           대기열이 빠르게 줄어듦:
           10,000 → 6,500 → 3,000...

────────────────────────────────────────────────────────────
20:06      15,000명    0명!       7대    3,500명/분    해소!
           ↓
           ✅ 대기열 완전 해소!

           이제 새로 오는 사람들:
           → 바로 입장!
```

---

## 🔍 상세 동작 과정

### 1단계: 트래픽 폭주 (19:58)

**10,000명이 동시 접속!**

```javascript
// 백엔드에서 매 요청마다 체크
app.get('/api/events/:id', async (req, res) => {
  const eventId = req.params.id;
  const userId = req.user.id;

  // 1. 현재 활성 사용자 수 확인
  const activeUsers = await redis.scard(`active:${eventId}`);
  const threshold = 1,000; // 임계값

  if (activeUsers >= threshold) {
    // 2. 대기열로!
    await redis.zadd(`queue:${eventId}`, Date.now(), userId);
    const position = await redis.zrank(`queue:${eventId}`, userId);

    return res.json({
      status: 'queued',
      position: position + 1,
      message: '현재 대기 중입니다'
    });
  }

  // 3. 임계값 미만이면 바로 입장
  await redis.sadd(`active:${eventId}`, userId);
  return res.json({
    status: 'active',
    message: '입장 완료'
  });
});
```

**결과:**
```
앞 1,000명: 바로 입장 ✅
  → Redis에 `active:event-123` Set에 추가
  → 이벤트 페이지 정상 접근
  → 티켓 구매 가능

나머지 9,000명: 대기열 ⏳
  → Redis에 `queue:event-123` Sorted Set에 추가
  → 프론트엔드에 대기열 모달 표시
  → "현재 8,245번째로 대기 중입니다"
```

### 2단계: Lambda가 대기열 감지 (19:58:30)

**Lambda 함수가 1분마다 실행 중:**

```javascript
// Lambda: queue-monitor.js
exports.handler = async () => {
  const queueSize = await redis.zcard('queue:event-123');

  console.log('대기열 크기:', queueSize); // 8,000명

  // CloudWatch로 메트릭 전송
  await cloudwatch.putMetricData({
    Namespace: 'Tiketi/Queue',
    MetricData: [{
      MetricName: 'QueueSize',
      Value: 8000,
      Unit: 'Count'
    }]
  });
};
```

**CloudWatch Alarm 발동:**
```
조건: QueueSize > 5,000
현재: 8,000
→ 🚨 알람 발동!
→ Auto Scaling Policy 실행
```

**Auto Scaling 실행:**
```
현재 EC2: 2대
목표 EC2: 7대 (5대 추가)

→ 5개 EC2 인스턴스 시작 명령
→ 예상 소요 시간: 3~5분
```

### 3단계: EC2가 대기열 처리 (19:58 ~ 20:03)

**기존 2대가 계속 처리:**

```javascript
// 백엔드에서 1초마다 실행되는 대기열 프로세서
setInterval(async () => {
  const eventId = 'event-123';

  // 1. 현재 활성 사용자 수
  const activeUsers = await redis.scard(`active:${eventId}`);
  const threshold = 1,000;

  // 2. 빈 자리 계산
  const availableSlots = threshold - activeUsers;

  if (availableSlots > 0) {
    // 3. 대기열에서 앞에서부터 꺼내기
    const users = await redis.zrange(`queue:${eventId}`, 0, availableSlots - 1);

    console.log(`${users.length}명 입장 허용`);

    for (const userId of users) {
      // 4. 활성 사용자로 등록
      await redis.sadd(`active:${eventId}`, userId);

      // 5. WebSocket으로 알림
      io.to(`user:${userId}`).emit('queue-enter', {
        message: '입장이 허용되었습니다!'
      });
    }

    // 6. 대기열에서 제거
    await redis.zremrangebyrank(`queue:${eventId}`, 0, availableSlots - 1);
  }
}, 1000); // 1초마다
```

**실제 처리 과정:**

```
19:58:00 - 대기열: 9,000명
           EC2 2대가 처리 중
           1초마다 약 20명씩 입장

19:58:01 - 대기열: 8,980명 (20명 입장)
19:58:02 - 대기열: 8,960명 (20명 입장)
19:58:03 - 대기열: 8,940명 (20명 입장)
...

20:00:00 - 대기열: 10,000명 (새 사용자 더 몰림)
           EC2 2대 계속 처리 중

20:03:00 - 🎉 EC2 5대 추가 완료!
           이제 7대가 처리
           1초마다 약 70명씩 입장

20:03:01 - 대기열: 9,930명 (70명 입장)
20:03:02 - 대기열: 9,860명 (70명 입장)
20:03:03 - 대기열: 9,790명 (70명 입장)
...

20:06:00 - 대기열: 0명! ✅
           모두 입장 완료!
```

### 4단계: 사용자 경험

**대기열에 있는 사용자의 화면:**

```javascript
// 프론트엔드: WaitingRoomModal.js
socket.on('queue-position-update', ({ position, total, estimatedWait }) => {
  setQueuePosition(position);
  setTotalQueue(total);
  setEstimatedWait(estimatedWait);
});

// 화면 표시:
┌─────────────────────────────────────┐
│   ⏳ 대기열 입장                     │
├─────────────────────────────────────┤
│                                     │
│   현재 대기 순번: 8,245 / 10,000    │
│                                     │
│   ▓▓▓▓▓▓▓▓▓░░░░░░░░░ 82%           │
│                                     │
│   예상 대기 시간: 약 3분             │
│                                     │
│   💡 EC2 서버가 추가되고 있어요!    │
│      곧 더 빠르게 입장됩니다        │
│                                     │
└─────────────────────────────────────┘

// 실시간 업데이트:
19:58:30 - "8,245번째 대기 중 (예상 4분)"
19:59:00 - "7,640번째 대기 중 (예상 3분 30초)"
19:59:30 - "7,035번째 대기 중 (예상 3분)"
20:00:00 - "6,430번째 대기 중 (예상 2분 30초)"
20:03:00 - "3,000번째 대기 중 (예상 40초)" ← EC2 추가 완료!
20:03:40 - "입장 허용되었습니다!" ✅
```

---

## 💡 핵심 개념 정리

### Q1: 대기열은 트래픽을 막는 건가요?

**A: 아니요! 트래픽을 "버퍼링"하는 거예요.**

```
트래픽을 막는다 (X):
  사용자 → 서버 → "지금 바쁘니까 나중에 와" → 연결 끊김

트래픽을 버퍼링한다 (O):
  사용자 → 서버 → "잠시만 기다려, 순번은 8,245번이야"
  → 연결 유지
  → 순차적으로 처리
  → 공정하게 입장
```

### Q2: 대기열에 있으면 아무것도 못 하나요?

**A: WebSocket 연결은 살아있어요!**

```
대기열에 있어도:
✅ WebSocket 연결 유지
✅ 실시간 순번 업데이트 받음
✅ 서버 상태 확인 가능
✅ 페이지 새로고침해도 순번 유지

못 하는 것:
❌ 티켓 구매
❌ 좌석 선택
❌ 이벤트 상세 조회 (제한적)
```

### Q3: EC2가 추가되면 대기열을 어떻게 나눠주나요?

**A: 나눠주는 게 아니라, 더 빠르게 꺼내요!**

```
Before (EC2 2대):
  1초에 20명씩 처리
  10,000명 → 500초 (8분) 소요

After (EC2 7대):
  1초에 70명씩 처리
  10,000명 → 143초 (2분 30초) 소요

→ 3.5배 빠르게!
```

**Redis 대기열은 하나예요:**
```
Redis: queue:event-123
  [1번, 2번, 3번, ..., 10,000번]

EC2-1: "1~10번 꺼낼게"
EC2-2: "11~20번 꺼낼게"
EC2-3: "21~30번 꺼낼게" ← 새로 추가된 EC2
...
EC2-7: "61~70번 꺼낼게" ← 새로 추가된 EC2

→ 동시에 여러 대가 처리
→ 순서는 보장 (FIFO)
```

### Q4: 대기열이 꽉 차면 어떻게 되나요?

**A: 대기열 크기는 무제한이에요!**

```
Redis Sorted Set:
  이론적 최대: 2^32 (약 42억 개)

실제로는:
  10만 명 정도면 이미 엄청난 트래픽
  100만 명은 거의 불가능한 수준

만약 정말로 10만 명이 대기열에 쌓이면:
  → Auto Scaling이 EC2를 최대 10대까지 확장
  → 분당 5,000명씩 처리
  → 20분이면 모두 입장
```

---

## 🎯 왜 이 방식이 좋은가?

### 1️⃣ 서버 보호

```
대기열 없이 (X):
  10,000명이 동시에 API 요청
  → EC2 2대가 처리 못 함
  → 서버 과부하
  → 타임아웃, 에러 폭발
  → 아무도 못 들어감 💥

대기열 사용 (O):
  10,000명 중 1,000명만 활성
  → EC2 2대가 여유롭게 처리
  → 나머지는 대기열에서 질서있게 대기
  → 모두 공정하게 입장 ✅
```

### 2️⃣ 공정성 보장

```
대기열 없이 (X):
  → 빠른 사람만 성공
  → 느린 사람은 에러
  → 불공평!

대기열 사용 (O):
  → 먼저 온 사람이 먼저 입장 (FIFO)
  → 순번 보장
  → 공정함!
```

### 3️⃣ 예측 가능성

```
대기열로:
  - 현재 대기 인원 파악
  - 예상 대기 시간 계산
  - 사용자에게 정확한 정보 제공

사용자 경험:
  "예상 3분 대기" (명확함)
  vs
  "계속 로딩 중..." (불안함)
```

### 4️⃣ 사전 확장

```
CPU 기반 Auto Scaling (X):
  CPU 80% 넘으면 확장
  → 이미 서버가 터지는 중
  → 사후 대응

대기열 기반 Auto Scaling (O):
  대기열 5,000명 넘으면 확장
  → 아직 서버는 여유
  → 사전 예방
```

---

## 📊 실제 데이터로 보는 효과

### 시나리오: 10,000명 동시 접속

**방법 1: 대기열 없이 EC2 10대 24/7 운영**
```
EC2: 10대 × ₩40,000 = ₩400,000/월
트래픽 처리: 즉시 (0초 대기)
사용자 경험: ⭐⭐⭐⭐⭐
비용: 💸💸💸💸💸
```

**방법 2: 대기열 + Auto Scaling**
```
EC2: 2대 → 7대 (3분 소요)
       ↓
평소: 2대 × ₩40,000 = ₩80,000/월
피크: 5대 추가 × 1시간 × 20회 = ₩9,000/월
합계: ₩89,000/월

트래픽 처리: 3분 대기 → 이후 빠르게 처리
사용자 경험: ⭐⭐⭐⭐ (3분 대기)
비용: 💸 (78% 절감!)
```

**결론:**
- 3분 대기를 감수하면 월 ₩311,000 절감
- 대부분의 티켓팅은 3분 대기 충분히 허용 가능
- 공정성 + 안정성 + 비용 효율 모두 확보!

---

## 🎬 마무리

대기열 기반 Auto Scaling은:

1. **트래픽을 막는 게 아니라 버퍼링**해요
2. **EC2가 추가되는 3~5분 동안** 대기열이 요청을 쌓아둬요
3. **EC2가 준비되면** 대기열에서 순차적으로 꺼내서 처리해요
4. **순서는 보장**되고, **서버는 안정적**이고, **비용은 저렴**해요!

은행 번호표와 똑같아요:
- 사람 많으면 번호표 받고 대기
- 창구 늘리는 동안 번호표로 질서 유지
- 창구 추가되면 빠르게 처리
- 공평하고 안정적!

질문 있으시면 말씀해주세요! 😊
